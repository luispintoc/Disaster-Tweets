{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIdcTJtkB9LZ"
   },
   "source": [
    "# NLP for disaster tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R89gQVg-uweL"
   },
   "source": [
    "## Outline\n",
    "- [0. Overview](#0)\n",
    "- [1. Read data and expand dataset](#1)\n",
    "- [2. Preprocessing](#2)\n",
    "- [3. BERT](#3)\n",
    "  - [3.1 Tokenization](#3-1)\n",
    "  - [3.2 BERT model](#3-2)\n",
    "- [4. GloVe Bi-LSTM](#4)\n",
    "  - [4.1 Tokenization](#4-1)\n",
    "  - [4.2 GloVe embeddings](#4-2)\n",
    "  - [4.3 LSTM model](#4-3)\n",
    "- [5. NB classifier + Tf-idf features](#5)\n",
    "- [6. Ensemble](#6)\n",
    "- [7. TO DO](#7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbrEJCr5tN4H"
   },
   "source": [
    "<a name='0'></a>\n",
    "# 0. Overview\n",
    "Natural language processing is used to tackle the problem of sentence classification, specifically, to classify whether a tweet is about a disaster or not. The following model scores on the top 10% of all submissions to the Kaggle competition with a final leaderboard F1 score of 0.84094."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUZxgMtEDW56"
   },
   "source": [
    "Uncomment the cell below if using Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgVSbU2AqK29"
   },
   "outputs": [],
   "source": [
    "#!pip install sentencepiece\n",
    "#import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ya3hlLOY3k8F"
   },
   "source": [
    "Import libraries, together with auxiliary scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SdA0CChHDbV9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime, sys, string\n",
    "import regex as re\n",
    "from random import randint\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, SpatialDropout1D, Dropout, Input, GlobalAveragePooling1D, Concatenate, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Auxiliary scripts\n",
    "import tokenization\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfPytuk8uc7h"
   },
   "source": [
    "<a name='1'></a>\n",
    "# 1. Read data and expand dataset\n",
    "Each sample in the train and test set has the following information:\n",
    "\n",
    "\n",
    "*   The **text** of a tweet\n",
    "*   A **keyword** from that tweet\n",
    "*   The **location** the tweet was sent from\n",
    "*   Label\n",
    "\n",
    "In this script, only the text was used, ignoring the other two given features.\n",
    "Moreover, some of the tweets were mislabelled and this is addressed below (by changing to the correct label).\n",
    "Furthermore, the train set was expand using additional tweets chosen at random from the following dataset: [link](https://www.kaggle.com/kazanova/sentiment140). These 1000 extra tweets were inspected by hand and labelled accordingly. It turns out that all these tweets are label 0.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G3yvgW3iM136",
    "outputId": "acd54e09-f575-47c1-d851-2fdab0fa4c44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (8604, 6)\n",
      "Test shape:  (3263, 4)\n",
      "Some tweet examples: \n",
      " ['Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'\n",
      " 'Forest fire near La Ronge Sask. Canada'\n",
      " \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\"\n",
      " '13,000 people receive #wildfires evacuation orders in California '\n",
      " 'Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school '\n",
      " '#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires'\n",
      " '#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas'\n",
      " \"I'm on top of the hill and I can see a fire in the woods...\"\n",
      " \"There's an emergency evacuation happening now in the building across the street\"\n",
      " \"I'm afraid that the tornado is coming to our area...\"]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Change target value to some mislabelled tweets\n",
    "ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n",
    "train.loc[train['id'].isin(ids_with_target_error),'target'] = 0\n",
    "\n",
    "# Expand the training set by adding tweets from: https://www.kaggle.com/kazanova/sentiment140\n",
    "extra_train = pd.read_csv('expand_train_dataset.csv') #They are all label 0\n",
    "train = train.append(extra_train, sort=False).reset_index()\n",
    "y_train = train.target.values\n",
    "print('Train shape: ', train.shape)\n",
    "print('Test shape: ', test.shape)\n",
    "print('Some tweet examples: \\n', train.text.values[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTxPILq_v6w0"
   },
   "source": [
    "<a name='2'></a>\n",
    "# 2. Preprocessing\n",
    "Since the dataset consists of raw text, it is necessary to 'clean' the text into a more suitable form since the models will use it as input. The preprocessing is separated into two functions:\n",
    "\n",
    "- preprocessing: Its main function is to correct mispelled words using a dictionary stored in utils.py\n",
    "- glove_preprocessing: This is the python version for the Ruby script created by the GloVe project at Stanford to preprocess Twitter data ([link](https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb)). It takes care of URLS, hashtags, usernames and others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DZ-jjUc9v_0j"
   },
   "outputs": [],
   "source": [
    "def preprocessing(tweet):\n",
    "  # Remove empty spaces\n",
    "    tweet = tweet.strip(' ')\n",
    "  # Remove old RT style\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "  # Tokenize to take care of mispelled words\n",
    "    tokenizer = TweetTokenizer(preserve_case=True, strip_handles=False,\n",
    "                               reduce_len=False)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    tweet_clean = ''\n",
    "  # Iterate over dict in utils.py to correct mispelled words\n",
    "    for word in tweet_tokens:\n",
    "        if word.lower() in mispell_dict.keys():\n",
    "            word = mispell_dict[word.lower()].lower()\n",
    "        if (word not in stop):\n",
    "            tweet_clean+= (' '+ word)\n",
    "\n",
    "    return tweet_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Dt4RNc7owp4Q"
   },
   "outputs": [],
   "source": [
    "FLAGS = re.MULTILINE | re.DOTALL\n",
    "\n",
    "def hashtag(text):\n",
    "    text = text.group()\n",
    "    hashtag_body = text[1:]\n",
    "    if hashtag_body.isupper():\n",
    "        result = \"<hashtag> {} <allcaps>\".format(hashtag_body.lower())\n",
    "    else:\n",
    "        result = \" \".join([\"<hashtag>\"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
    "    return result\n",
    "\n",
    "def allcaps(text):\n",
    "    text = text.group()\n",
    "    return text.lower() + \" <allcaps> \"\n",
    "\n",
    "def glove_preprocessing(text):\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"<3\",\"<heart>\")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "    text = re_sub(r\"#\\w+\", hashtag)\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "\n",
    "    text = re_sub(r\"([a-zA-Z<>()])([?!.:;,])\", r\"\\1 \\2\")\n",
    "    text = re_sub(r\"\\(([a-zA-Z<>]+)\\)\", r\"( \\1 )\")\n",
    "    text = re_sub(r\"  \", r\" \")\n",
    "    text = re_sub(r\" ([A-Z]){2,} \", allcaps)\n",
    "    \n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_m2n5rdw3r6"
   },
   "source": [
    "Apply preprocessing functions to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3QBHgQgNKMi"
   },
   "outputs": [],
   "source": [
    "tweets = train['text']\n",
    "tweets_test = test['text']\n",
    "\n",
    "for i, line in enumerate(tweets):\n",
    "    pre_tweet = preprocessing(line)\n",
    "    tweets[i] = glove_preprocessing(pre_tweet)\n",
    "\n",
    "for i, line in enumerate(tweets_test):\n",
    "    pre_tweet_test = preprocessing(line)\n",
    "    tweets_test[i] = glove_preprocessing(pre_tweet_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5HPgyVFymQn"
   },
   "source": [
    "<a name='3'></a>\n",
    "# 3. BERT\n",
    "The Bidirectional Encoder Representations from Transformers (BERT) is a powerful transformer (encoder) that produces SOTA results in a variety of NLP tasks. A pre-trained version of BERT base will be used from TensorFlow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0QsVpvgaTEpk"
   },
   "outputs": [],
   "source": [
    "# Load BERT\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QynvLNpTzZub"
   },
   "source": [
    "<a name='3-1'></a>\n",
    "#### 3.1 Tokenization\n",
    "The first step is to use the BERT tokenizer (that can be found in the tokenization.py script) to first split the word into tokens. Next, we add the special tokens needed for sentence classifications (these are [CLS] at the first position, and [SEP] at the end of the sentence). The tokens are then replaced by unique ids from the embedding table given by the model. Also, BERT works with a constant input lenght which means that if the sentence is shorter that this hyperparam, it will be padded with 0s until the assigned lenght. Conversely, if the lenght of the sentence is too large, it will be truncated to the give parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nfx4vS-m8Bk0"
   },
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "          \n",
    "        # Truncate text\n",
    "        text = text[:max_len-2]\n",
    "\n",
    "        # Add special tokens\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        # Look-up the value of each token in the embedding table\n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "605ZOxJbUVTK"
   },
   "outputs": [],
   "source": [
    "max_len = 32\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "bert_train_input = bert_encode(train.text.values, tokenizer, max_len=max_len)\n",
    "bert_test_input = bert_encode(test.text.values, tokenizer, max_len=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jSI_0J-zt6N"
   },
   "source": [
    "<a name='3-2'></a>\n",
    "#### 3.2 BERT model\n",
    "The BERT model expects three inputs, these are produced in the bert_encode function above. For this specific task, the two outputs of the BERT model are concatenated to later be fed to a Dense layer for classification. The two outputs are: BERT's output for the [CLS] token and the output for the embedding tokens. Since the output is a 3d tensor, a global average pooling is perform in the sentence lenght direction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UPQd8Ks0Uyj1",
    "outputId": "b274dd07-55fb-43f9-99b8-f41989f0bcf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 32)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 32)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 32)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 768)          0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 768)          0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1536)         0           tf.__operators__.getitem[0][0]   \n",
      "                                                                 global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1536)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1537        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,777\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "_, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "cls_feat = sequence_output[:, 0, :]\n",
    "emb_feat = GlobalAveragePooling1D()(sequence_output)\n",
    "x = Concatenate()([cls_feat, emb_feat])\n",
    "x = Dropout(0.3)(x)\n",
    "out = Dense(1, activation='sigmoid')(x)\n",
    "bert_model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MTl-fzw6U3Sc",
    "outputId": "1b7f66a6-f228-4035-ccbc-0a88bb34f169"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "216/216 - 64s - loss: 0.6358 - accuracy: 0.6436 - val_loss: 0.4059 - val_accuracy: 0.8908\n",
      "Epoch 2/8\n",
      "216/216 - 51s - loss: 0.5097 - accuracy: 0.7588 - val_loss: 0.3011 - val_accuracy: 0.9082\n",
      "Epoch 3/8\n",
      "216/216 - 52s - loss: 0.4569 - accuracy: 0.7943 - val_loss: 0.2756 - val_accuracy: 0.9123\n",
      "Epoch 4/8\n",
      "216/216 - 53s - loss: 0.4258 - accuracy: 0.8158 - val_loss: 0.2779 - val_accuracy: 0.9123\n",
      "Epoch 5/8\n",
      "216/216 - 53s - loss: 0.4086 - accuracy: 0.8194 - val_loss: 0.2782 - val_accuracy: 0.9059\n",
      "Epoch 6/8\n",
      "216/216 - 53s - loss: 0.3924 - accuracy: 0.8322 - val_loss: 0.2832 - val_accuracy: 0.9064\n",
      "Epoch 7/8\n",
      "216/216 - 53s - loss: 0.3832 - accuracy: 0.8357 - val_loss: 0.2494 - val_accuracy: 0.9140\n",
      "Epoch 8/8\n",
      "216/216 - 53s - loss: 0.3740 - accuracy: 0.8437 - val_loss: 0.2501 - val_accuracy: 0.9146\n"
     ]
    }
   ],
   "source": [
    "bert_model.compile(Adam(lr=1e-6), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "train_history = bert_model.fit(\n",
    "    bert_train_input, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=8,\n",
    "    batch_size=32,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTt7BIyV1pWM"
   },
   "source": [
    "Create features from the trained BERT model to later feed the ensemble algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "R3807UJY2Cq1"
   },
   "outputs": [],
   "source": [
    "bert_feat = bert_model.predict(bert_train_input).flatten()\n",
    "bert_out = bert_model.predict(bert_test_input).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timSJe632cPt"
   },
   "source": [
    "<a name='4'></a>\n",
    "# 4. GloVe Bi-LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U_ZDHPi2mjj"
   },
   "source": [
    "<a name='4-1'></a>\n",
    "#### 4.1 Tokenization\n",
    "Similar to BERT, the LSTM model also expects ids instead of words as inputs. This is performed using the TensorFlow tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02AtSCQ348hA",
    "outputId": "533d5492-398a-49da-f418-0fea3b1c5eea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words:  15241\n"
     ]
    }
   ],
   "source": [
    "tokenizer_glove = Tokenizer(split=' ', oov_token='<UNK>')\n",
    "tokenizer_glove.fit_on_texts(tweets)\n",
    "glove_x = tokenizer_glove.texts_to_sequences(tweets)\n",
    "word_index = tokenizer_glove.word_index\n",
    "print('Number of unique words: ', len(word_index))\n",
    "glove_x = sequence.pad_sequences(glove_x)\n",
    "glove_x_test = tokenizer_glove.texts_to_sequences(tweets_test)\n",
    "glove_x_test = sequence.pad_sequences(glove_x_test, maxlen=np.shape(glove_x)[1])\n",
    "Y = pd.get_dummies(y_train).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXfX7YPb4plJ"
   },
   "source": [
    "<a name='4-2'></a>\n",
    "#### 4.2 GloVe embeddings\n",
    "The GloVe embeddings were pre-trained on 2 billion tweets and will be used as inputs to the model. The length of each embedding is 200, meaning that each word is represented by 200 floats. Moreover, an embedding matrix will be created for our corpus to map between ids and embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-55-Kpr5AeY",
    "outputId": "93d7ef0d-b4ed-4bb0-8ce9-c91c5da98adc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15241/15241 [00:00<00:00, 493289.51it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_dict = {}\n",
    "\n",
    "with open('glove.twitter.27B.200d.txt', encoding=\"utf8\") as glove:\n",
    "    for line in glove:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vectors = np.asarray(values[1:], 'float32')\n",
    "        embedding_dict[word] = vectors        \n",
    "glove.close()\n",
    "\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words,200))\n",
    "\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embedding_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xjn1GR45pzK"
   },
   "source": [
    "<a name='4-3'></a>\n",
    "#### 4.2 LSTM model\n",
    "Long Short-Term Memory (LSTM) models are a type of recurrent neural network that allows for longer range dependencies, unlike traditional RNNs. For this task, a bi-directional LSTM will be used which can levarage from information from both past and future to create the output of the current timepoint. The LSTM has 512 hidden states on each direction. Dropout and recurrent dropout are added for regularization. Also spatial dropout for the embedding features are used for the same reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WS6kjrYD512r",
    "outputId": "0578500e-8e78-4f6b-924b-46ea3d900081"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 43, 200)           3048400   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 43, 200)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 1024)              2920448   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 6,034,578\n",
      "Trainable params: 6,034,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "glove_model = Sequential()\n",
    "glove_model.add(Embedding(num_words, 200, input_length = np.shape(glove_x)[1], embeddings_initializer=Constant(embedding_matrix), trainable=True))\n",
    "glove_model.add(SpatialDropout1D(0.1))\n",
    "glove_model.add(Bidirectional(LSTM(512, dropout=0.1, recurrent_dropout=0.2)))\n",
    "glove_model.add(Dense(64, activation = 'relu'))\n",
    "glove_model.add(Dropout(0.2))\n",
    "glove_model.add(Dense(2, activation='sigmoid'))\n",
    "glove_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14Drn5KF6QtR",
    "outputId": "c379b2df-0d22-4010-e249-556a6220750f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "108/108 - 43s - loss: 0.5978 - accuracy: 0.6924 - val_loss: 0.5057 - val_accuracy: 0.7775\n",
      "Epoch 2/10\n",
      "108/108 - 39s - loss: 0.4454 - accuracy: 0.8043 - val_loss: 0.4221 - val_accuracy: 0.8175\n",
      "Epoch 3/10\n",
      "108/108 - 39s - loss: 0.4070 - accuracy: 0.8241 - val_loss: 0.4069 - val_accuracy: 0.8257\n",
      "Epoch 4/10\n",
      "108/108 - 39s - loss: 0.3961 - accuracy: 0.8297 - val_loss: 0.4043 - val_accuracy: 0.8222\n",
      "Epoch 5/10\n",
      "108/108 - 39s - loss: 0.3848 - accuracy: 0.8320 - val_loss: 0.3952 - val_accuracy: 0.8239\n",
      "Epoch 6/10\n",
      "108/108 - 39s - loss: 0.3773 - accuracy: 0.8361 - val_loss: 0.3933 - val_accuracy: 0.8234\n",
      "Epoch 7/10\n",
      "108/108 - 39s - loss: 0.3744 - accuracy: 0.8350 - val_loss: 0.3934 - val_accuracy: 0.8222\n",
      "Epoch 8/10\n",
      "108/108 - 39s - loss: 0.3697 - accuracy: 0.8402 - val_loss: 0.3909 - val_accuracy: 0.8263\n",
      "Epoch 9/10\n",
      "108/108 - 39s - loss: 0.3683 - accuracy: 0.8389 - val_loss: 0.3900 - val_accuracy: 0.8251\n",
      "Epoch 10/10\n",
      "108/108 - 39s - loss: 0.3674 - accuracy: 0.8443 - val_loss: 0.3892 - val_accuracy: 0.8268\n"
     ]
    }
   ],
   "source": [
    "x_1, x_val, y_1, y_val = train_test_split(glove_x, Y, test_size=0.2, random_state=40, stratify=y_train)\n",
    "glove_model.compile(Adam(lr=2e-5), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "train_history = glove_model.fit(x_1, y_1, \n",
    "                epochs=10, \n",
    "                batch_size=64, \n",
    "                verbose=2, \n",
    "                validation_data=(x_val, y_val)\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zl_Azs5rl_u"
   },
   "source": [
    "Create second set of features using trained LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "HGGtQGYbrd__"
   },
   "outputs": [],
   "source": [
    "glove_feat = glove_model.predict(glove_x)[:,1]\n",
    "glove_out = glove_model.predict(glove_x_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yO6ZfiT7rs1y"
   },
   "source": [
    "<a name='5'></a>\n",
    "# 5. NB classifier + Tf-idf features\n",
    "The Naive Bayes classifier is used together with tf-idf features to produce the last set of features. Tf-idf is a variant of the bag-of-words model that calculates the importance of each word by taking the raw frequencies of ocurrences in a document and scales them down by their frequency in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LMrdabaxr5Xz"
   },
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(max_features=2500, stop_words=stop).fit(tweets)\n",
    "x_train_tf = tf.transform(tweets)\n",
    "x_test_tf = tf.transform(tweets_test)\n",
    "\n",
    "NB = MultinomialNB(alpha=1).fit(x_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJ3L1bajscXz"
   },
   "source": [
    "Create thrid set of features using the trained NB classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "nX1i1oVyschd"
   },
   "outputs": [],
   "source": [
    "NB_tfidf_feat = NB.predict_proba(x_train_tf)[:,1]\n",
    "NB_tfidf_out = NB.predict_proba(x_test_tf)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UY8I8mkEsvRz"
   },
   "source": [
    "<a name='6'></a>\n",
    "# 6. Ensemble\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9x2DmT8atTLB"
   },
   "source": [
    "A L2 regression model is used to combine the prediction of the models described above. A high regularization is used for the model not to be biased towards one single model and it can generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "eUc3ZlX8szmT"
   },
   "outputs": [],
   "source": [
    "feat_train = pd.DataFrame({'bert':bert_feat, 'lstm_glove':glove_feat, 'nb':NB_tfidf_feat})\n",
    "feat_test = pd.DataFrame({'bert':bert_out, 'lstm_glove':glove_out, 'nb':NB_tfidf_out})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "nI-wv-UDtZ_v"
   },
   "outputs": [],
   "source": [
    "ensemble_model = Ridge(alpha=10)\n",
    "predictions = ensemble_model.fit(feat_train, y_train).predict(feat_test).round().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38iHoLSgv0uA"
   },
   "source": [
    "Create csv for competition submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m14mJiCbtdlk",
    "outputId": "7360f8b8-4371-4e02-970c-3e795e530c3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  target\n",
      "0   0       1\n",
      "1   2       1\n",
      "2   3       1\n",
      "3   9       1\n",
      "4  11       1\n",
      "5  12       1\n",
      "6  21       0\n",
      "7  22       0\n",
      "8  27       0\n",
      "9  29       0\n"
     ]
    }
   ],
   "source": [
    "name = 'final_ensemble'\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission['target'] = predictions\n",
    "print(submission.head(10))\n",
    "submission.to_csv('submit_'+name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUTa9LgT275g"
   },
   "source": [
    "<a name='7'></a>\n",
    "# 7. TO DO\n",
    "\n",
    "\n",
    "*   Introduce the other two features (location and keywords) to the models\n",
    "*   Expand the preprocessing step (cleaning of data)\n",
    "*   Create meta-features from the tweets that might boost performance\n",
    "*   Use other pre-trained embeddings that might be more useful in this task\n",
    "*   Use other SOTA algorithms like BERT Large, T5, GPT2 and others.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
